mkdir ckpt
cd ckpt
wget https://huggingface.co/ByteDance/HLLM/resolve/main/1B_Pixel8M/pytorch_model.bin

cd code
python3 main.py \
--config_file overall/LLM_deepspeed.yaml HLLM/HLLM.yaml \
--loss nce \
--epochs 3 \
--dataset Pixel200K \
--train_batch_size 4 \
--gradient_accumulation_steps 4 \
--MAX_TEXT_LENGTH 64 \
--MAX_ITEM_LIST_LENGTH 10 \
--checkpoint_dir ../ckpt \
--optim_args.learning_rate 3e-5 \
--item_pretrain_dir TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T \
--user_pretrain_dir TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T \
--text_path /root/HLLM/information \
--use_trained_model True \
--use_LoRA True \
--LoRA_Rank 16 \
--LoRA_Alpha 32 \
--debug False