Sat 29 Mar 2025 04:48:59 INFO  
General Hyper Parameters:
seed = 2020
state = INFO
use_text = True
reproducibility = True
checkpoint_dir = ../ckpt
show_progress = True
log_wandb = False
data_path = ../dataset/
strategy = deepspeed
precision = bf16-mixed
model = HLLM

Training Hyper Parameters:
epochs = 3
train_batch_size = 4
optim_args = {'learning_rate': 1e-05, 'weight_decay': 0.01}
eval_step = 1
stopping_step = 5

Evaluation Hyper Parameters:
eval_batch_size = 256
topk = [5, 10, 50, 200]
metrics = ['Recall', 'NDCG']
valid_metric = NDCG@200
metric_decimal_place = 7
eval_type = EvaluatorType.RANKING
valid_metric_bigger = True

Dataset Hyper Parameters:
MAX_ITEM_LIST_LENGTH = 10
MAX_TEXT_LENGTH = 64
text_keys = ['title', 'tag', 'description']
item_prompt = Compress the following sentence into embedding: 

LoRA Hyper Parameters:
use_LoRA = False
LoRA_Rank = 8
LoRA_Alpha = 16
LoRA_Dropout = 0

Other Hyper Parameters: 
wandb_project = REC
text_path = /root/HLLM/information/Pixel200K.csv
item_emb_token_n = 1
loss = nce
scheduler_args = {'type': 'cosine', 'warmup': 0.1}
stage = 2
item_pretrain_dir = TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T
item_llm_init = True
user_pretrain_dir = TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T
user_llm_init = True
use_ft_flash_attn = True
gradient_accumulation_steps = 4
use_tained_model = True
MODEL_INPUT_TYPE = InputType.SEQ
device = cuda:0
use_trained_model = True


Sat 29 Mar 2025 04:48:59 INFO  Pixel200K
The number of users: 200001
Average actions of users: 19.82828
The number of items: 96283
Average actions of items: 41.187927130720176
The number of inters: 3965656
The sparsity of the dataset: 99.9794063532928%
Sat 29 Mar 2025 04:48:59 INFO  HLLM(
  (item_llm): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(32000, 2048)
      (layers): ModuleList(
        (0-21): 22 x LlamaDecoderLayer(
          (self_attn): LlamaAttention(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=256, bias=False)
            (v_proj): Linear(in_features=2048, out_features=256, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
            (act_fn): SiLUActivation()
          )
          (input_layernorm): LlamaRMSNorm()
          (post_attention_layernorm): LlamaRMSNorm()
        )
      )
      (norm): LlamaRMSNorm()
    )
    (lm_head): Linear(in_features=2048, out_features=32000, bias=False)
  )
  (user_llm): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(32000, 2048)
      (layers): ModuleList(
        (0-21): 22 x LlamaDecoderLayer(
          (self_attn): LlamaAttention(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=256, bias=False)
            (v_proj): Linear(in_features=2048, out_features=256, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
            (act_fn): SiLUActivation()
          )
          (input_layernorm): LlamaRMSNorm()
          (post_attention_layernorm): LlamaRMSNorm()
        )
      )
      (norm): LlamaRMSNorm()
    )
    (lm_head): Linear(in_features=2048, out_features=32000, bias=False)
  )
  (item_proj): Identity()
)
Trainable parameters: 120064000
Sat 29 Mar 2025 04:48:59 INFO  Use consine scheduler with 11382.300000000001 warmup 113823 total steps
Sat 29 Mar 2025 04:48:59 INFO  Use deepspeed strategy
Sat 29 Mar 2025 04:49:07 INFO  lr: 0.0000000 accum_loss: 0.0149 data: 2.862 fwd: 0.889 bwd: 0.296 nce_samples: 89.000 nce_top1_acc: 0.703 nce_top5_acc: 0.892 nce_top10_acc: 0.919 nce_top50_acc: 1.000

Sat 29 Mar 2025 04:49:07 INFO  
--------------------------------------------------
Sat 29 Mar 2025 04:49:41 INFO  lr: 0.0000000 accum_loss: 2.2498 data: 0.051 fwd: 0.167 bwd: 0.231 nce_samples: 88.974 nce_top1_acc: 0.421 nce_top5_acc: 0.763 nce_top10_acc: 0.842 nce_top50_acc: 1.000

Sat 29 Mar 2025 04:49:41 INFO  
--------------------------------------------------
Sat 29 Mar 2025 04:50:15 INFO  lr: 0.0000000 accum_loss: 2.3056 data: 0.051 fwd: 0.166 bwd: 0.240 nce_samples: 89.000 nce_top1_acc: 0.686 nce_top5_acc: 0.829 nce_top10_acc: 0.914 nce_top50_acc: 1.000

Sat 29 Mar 2025 04:50:15 INFO  
--------------------------------------------------
Sat 29 Mar 2025 04:50:49 INFO  lr: 0.0000001 accum_loss: 2.2635 data: 0.053 fwd: 0.165 bwd: 0.229 nce_samples: 89.000 nce_top1_acc: 0.308 nce_top5_acc: 0.744 nce_top10_acc: 0.795 nce_top50_acc: 0.974

Sat 29 Mar 2025 04:50:49 INFO  
--------------------------------------------------
Sat 29 Mar 2025 04:51:23 INFO  lr: 0.0000001 accum_loss: 2.3476 data: 0.049 fwd: 0.166 bwd: 0.242 nce_samples: 89.000 nce_top1_acc: 0.441 nce_top5_acc: 0.735 nce_top10_acc: 0.824 nce_top50_acc: 1.000

Sat 29 Mar 2025 04:51:23 INFO  
--------------------------------------------------
